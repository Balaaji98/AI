{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeId5zFJkhsX",
        "outputId": "271115cd-57b9-42d3-c42a-7cf0bb683e83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: groq in /usr/local/lib/python3.11/dist-packages (0.30.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.14.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install groq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "import gradio as gr\n",
        "\n",
        "# Set your Groq API key\n",
        "client = Groq(api_key=\"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\")\n"
      ],
      "metadata": {
        "id": "3i1EE4PruwNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_with_llama3(prompt, history=None):\n",
        "    if history is None:\n",
        "        history = []\n",
        "\n",
        "    # Add the new user message to the conversation history\n",
        "    messages = history + [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "    # Make the API call\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"llama3-70b-8192\",  # âœ… Use a currently supported model\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "    # Extract the assistant's reply\n",
        "    reply = response.choices[0].message.content\n",
        "\n",
        "    # Update history with the new interaction\n",
        "    history.append({\"role\": \"user\", \"content\": prompt})\n",
        "    history.append({\"role\": \"assistant\", \"content\": reply})\n",
        "\n",
        "    return reply, history\n"
      ],
      "metadata": {
        "id": "MbVW2fXbu2n4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start with an empty chat history\n",
        "history = []\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() in {\"exit\", \"quit\"}:\n",
        "        break\n",
        "\n",
        "    reply, history = chat_with_model(user_input, history)\n",
        "    print(f\"Bot: {reply}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j70YlVyYu3-m",
        "outputId": "313f2887-30fd-47d2-fa34-84f9570ef296"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You: who are you\n",
            "Bot: I am LLaMA, an AI assistant developed by Meta AI that can understand and respond to human input in a conversational manner. I'm a large language model, trained on a massive dataset of text from the internet, which allows me to generate human-like responses to a wide range of topics and questions.\n",
            "\n",
            "I can chat with you about anything from science and history to entertainment and culture. I can also help with tasks such as language translation, text summarization, and even creative writing. My goal is to provide helpful and informative responses to your questions and engage in productive conversations.\n",
            "\n",
            "I'm constantly learning and improving, so please bear with me if I make any mistakes or don't quite understand what you're asking. I'm here to help and provide information to the best of my abilities!\n",
            "\n",
            "You: quit\n"
          ]
        }
      ]
    }
  ]
}